input {
  # beats {
  #  port => 5000
  #}
kafka {
                topic_id => 'logstash'
                zk_connect => 'zookeeper:2181'
                consumer_threads => 8
                decorate_events => true
        }
  }

filter {

# grok {
#	match => ["message", "%{SYSLOGTIMESTAMP:timestamp}\s+%{URIHOST}\s+%{JAVACLASS}\s+%{SYSLOGPROG}:\s*%{UUID:traceid}?\s*%{SYSLOG5424SD:time}?\s*(?<logtype>\b[_]\w+\b)?\s*%{LOGLEVEL:loglevel}?\s*%{GREEDYDATA:data}?"]
#	}

grok {
         match => ["message", "%{SYSLOGTIMESTAMP:timestamp} %{URIHOST} %{JAVACLASS} %{SYSLOGPROG}: %{GREEDYDATA:data}"]
         add_field => {"logstash_instance" => "logstash2" }
  }

  multiline {
    pattern => ": \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)\s\["
 negate => true
 what => "previous"
 stream_identity => "%{program}.%{source}.%{host}"
  }



  mutate {
	join => ["data", "
" ]
	}
  
  sequence {}

}

output {
  elasticsearch { hosts => ["elastic:9200"] }
  stdout { codec => rubydebug }
}
